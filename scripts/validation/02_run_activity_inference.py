#!/usr/bin/env python3
"""
Stage 2: Run Activity Inference on Pseudobulk Data
==================================================
Runs CytoSig/LinCytoSig/SecAct activity inference on pre-computed pseudobulk H5AD files.
Uses GPU-accelerated batch processing with streaming output.

Usage:
    python 02_run_activity_inference.py --pseudobulk cima_L1 --signature cytosig
    python 02_run_activity_inference.py --config-index 0
    python 02_run_activity_inference.py --all
"""

import argparse
import gc
import json
import sys
import time
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import anndata as ad
import h5py
from scipy import stats

# Add paths
sys.path.insert(0, '/data/parks34/projects/1ridgesig/SecActpy')

from secactpy import (
    load_cytosig, load_secact, load_lincytosig,
    ridge_batch, estimate_batch_size,
    CUPY_AVAILABLE
)

warnings.filterwarnings('ignore')

# ==============================================================================
# Configuration
# ==============================================================================

PSEUDOBULK_ROOT = Path('/data/parks34/projects/2cytoatlas/results/validation/pseudobulk')
OUTPUT_ROOT = Path('/data/parks34/projects/2cytoatlas/results/validation/activity')

# Pseudobulk files (generated by 01_generate_pseudobulk.py)
PSEUDOBULK_FILES = {
    'cima_L1': 'cima_L1_pseudobulk.h5ad',
    'cima_L2': 'cima_L2_pseudobulk.h5ad',
    'cima_L3': 'cima_L3_pseudobulk.h5ad',
    'inflammation_main_L1': 'inflammation_main_L1_pseudobulk.h5ad',
    'inflammation_main_L2': 'inflammation_main_L2_pseudobulk.h5ad',
    'inflammation_val_L1': 'inflammation_val_L1_pseudobulk.h5ad',
    'inflammation_val_L2': 'inflammation_val_L2_pseudobulk.h5ad',
    'scatlas_normal_organ_celltype': 'scatlas_normal_organ_celltype_pseudobulk.h5ad',
    'scatlas_normal_celltype': 'scatlas_normal_celltype_pseudobulk.h5ad',
    'scatlas_cancer_organ_celltype': 'scatlas_cancer_organ_celltype_pseudobulk.h5ad',
    'scatlas_cancer_celltype': 'scatlas_cancer_celltype_pseudobulk.h5ad',
}

SIGNATURES = ['cytosig', 'lincytosig', 'secact']

# Inference parameters
N_RAND = 1000
SEED = 42
BATCH_SIZE = 5000

# ==============================================================================
# Logging
# ==============================================================================

def log(msg: str):
    timestamp = time.strftime('%H:%M:%S')
    print(f"[{timestamp}] {msg}", flush=True)

# ==============================================================================
# Signature Loading
# ==============================================================================

def load_signature(name: str) -> pd.DataFrame:
    """Load signature matrix by name."""
    if name == 'cytosig':
        return load_cytosig()
    elif name == 'lincytosig':
        return load_lincytosig()
    elif name == 'secact':
        return load_secact()
    else:
        raise ValueError(f"Unknown signature: {name}")

# ==============================================================================
# Activity Inference
# ==============================================================================

def run_activity_inference(
    pseudobulk_path: Path,
    signature_name: str,
    output_path: Path,
    n_rand: int = N_RAND,
    seed: int = SEED,
    batch_size: int = BATCH_SIZE,
) -> Dict:
    """
    Run activity inference on pseudobulk data with streaming output.

    Args:
        pseudobulk_path: Path to pseudobulk H5AD file
        signature_name: Signature type (cytosig, lincytosig, secact)
        output_path: Path for output activity H5AD
        n_rand: Number of permutations
        seed: Random seed
        batch_size: Samples per batch

    Returns:
        Dict with summary statistics
    """
    log(f"Loading pseudobulk: {pseudobulk_path}")
    adata = ad.read_h5ad(pseudobulk_path)

    n_samples = adata.n_obs
    n_genes = adata.n_vars
    genes = list(adata.var_names)
    sample_names = list(adata.obs_names)

    log(f"  Shape: {adata.shape}")

    # Load signature
    log(f"Loading signature: {signature_name}")
    sig_matrix = load_signature(signature_name)
    log(f"  Signature shape: {sig_matrix.shape}")

    # Align genes
    common_genes = list(set(genes) & set(sig_matrix.index))
    log(f"  Common genes: {len(common_genes)}")

    if len(common_genes) < 100:
        raise ValueError(f"Too few common genes: {len(common_genes)}")

    # Get aligned data
    gene_idx = [genes.index(g) for g in common_genes]
    X = sig_matrix.loc[common_genes].values  # (genes × signatures)
    Y = adata.X[:, gene_idx].T  # (genes × samples)

    feature_names = list(sig_matrix.columns)
    n_features = len(feature_names)

    log(f"Running ridge regression ({n_samples} samples, {n_features} signatures)...")
    log(f"  Backend: {'CuPy (GPU)' if CUPY_AVAILABLE else 'NumPy (CPU)'}")
    log(f"  Batch size: {batch_size}")
    log(f"  n_rand: {n_rand}")

    # Prepare output path
    output_path.parent.mkdir(parents=True, exist_ok=True)
    h5_output = output_path.with_suffix('.h5')

    start_time = time.time()

    # Run with streaming output
    ridge_batch(
        X, Y,
        batch_size=batch_size,
        nrand=n_rand,
        seed=seed,
        output_path=str(h5_output),
        output_compression="gzip",
        feature_names=feature_names,
        sample_names=sample_names,
        verbose=True
    )

    inference_time = time.time() - start_time
    log(f"  Inference completed in {inference_time:.1f}s")

    # Load results and convert to AnnData
    log("Converting to AnnData format...")
    with h5py.File(h5_output, 'r') as f:
        zscore = f['zscore'][:]
        pvalue = f['pvalue'][:]

    # Create AnnData with activity (samples × signatures)
    activity_adata = ad.AnnData(
        X=zscore.T,  # (samples × signatures)
        obs=adata.obs.copy(),
        var=pd.DataFrame(index=feature_names),
    )
    activity_adata.layers['pvalue'] = pvalue.T

    # Save as H5AD
    activity_adata.write_h5ad(output_path, compression='gzip')
    log(f"  Saved: {output_path}")

    # Clean up intermediate H5
    h5_output.unlink()

    # Summary
    summary = {
        'pseudobulk': str(pseudobulk_path),
        'signature': signature_name,
        'n_samples': n_samples,
        'n_signatures': n_features,
        'n_common_genes': len(common_genes),
        'inference_time_seconds': inference_time,
        'output_path': str(output_path),
    }

    return summary


# ==============================================================================
# Validation (Expression vs Activity Correlation)
# ==============================================================================

def validate_expression_vs_activity(
    pseudobulk_path: Path,
    activity_path: Path,
    signature_name: str,
    output_path: Path,
    min_samples: int = 10,
) -> pd.DataFrame:
    """
    Validate activity by correlating with target gene expression.

    Returns:
        DataFrame with validation results per signature
    """
    log("Running validation (expression vs activity correlation)...")

    # Load data
    expr_adata = ad.read_h5ad(pseudobulk_path)
    act_adata = ad.read_h5ad(activity_path)

    expr_genes = list(expr_adata.var_names)
    expr_genes_upper = {g.upper(): g for g in expr_genes}

    # CytoSig name mapping
    CYTOSIG_TO_HGNC = {
        'TNFA': 'TNF', 'IFNA': 'IFNA1', 'IFNB': 'IFNB1', 'IFNL': 'IFNL1',
        'GMCSF': 'CSF2', 'GCSF': 'CSF3', 'MCSF': 'CSF1',
        'IL12': 'IL12A', 'Activin A': 'INHBA', 'TWEAK': 'TNFSF12',
        'CD40L': 'CD40LG', 'PDL1': 'CD274',
    }

    results = []

    for sig_idx, sig_name in enumerate(act_adata.var_names):
        # Determine target gene
        if signature_name == 'lincytosig' and '__' in sig_name:
            parts = sig_name.split('__')
            cell_type = parts[0]
            cytokine = parts[1]
            gene_name = CYTOSIG_TO_HGNC.get(cytokine, cytokine)

            # Filter to matching cell type
            if 'cell_type' in act_adata.obs.columns:
                mask = act_adata.obs['cell_type'] == cell_type
                valid_samples = act_adata.obs_names[mask].tolist()
            else:
                valid_samples = list(act_adata.obs_names)
        else:
            gene_name = CYTOSIG_TO_HGNC.get(sig_name, sig_name)
            valid_samples = list(act_adata.obs_names)
            cell_type = None
            cytokine = sig_name

        # Find gene in expression
        if gene_name.upper() not in expr_genes_upper:
            continue
        actual_gene = expr_genes_upper[gene_name.upper()]
        gene_idx = expr_genes.index(actual_gene)

        # Get common samples
        common_samples = [s for s in valid_samples if s in expr_adata.obs_names]
        if len(common_samples) < min_samples:
            continue

        # Get sample indices
        expr_sample_idx = [list(expr_adata.obs_names).index(s) for s in common_samples]
        act_sample_idx = [list(act_adata.obs_names).index(s) for s in common_samples]

        # Get values
        expr_vals = expr_adata.X[expr_sample_idx, gene_idx]
        act_vals = act_adata.X[act_sample_idx, sig_idx]

        if hasattr(expr_vals, 'toarray'):
            expr_vals = expr_vals.toarray().flatten()
        if hasattr(act_vals, 'toarray'):
            act_vals = act_vals.toarray().flatten()

        # Remove NaN
        mask = ~(np.isnan(expr_vals) | np.isnan(act_vals))
        if mask.sum() < min_samples:
            continue

        expr_vals = expr_vals[mask]
        act_vals = act_vals[mask]

        # Normalize expression: log1p then z-score
        expr_vals = np.log1p(expr_vals)
        expr_std = np.std(expr_vals)
        if expr_std > 0:
            expr_vals = (expr_vals - np.mean(expr_vals)) / expr_std

        # Correlations
        r_pearson, p_pearson = stats.pearsonr(expr_vals, act_vals)
        r_spearman, p_spearman = stats.spearmanr(expr_vals, act_vals)

        results.append({
            'signature': sig_name,
            'gene': actual_gene,
            'cell_type': cell_type,
            'cytokine': cytokine,
            'pearson_r': r_pearson,
            'pearson_p': p_pearson,
            'spearman_r': r_spearman,
            'spearman_p': p_spearman,
            'r2': r_pearson ** 2,
            'n_samples': len(expr_vals),
            'mean_expression': np.mean(expr_vals),
            'mean_activity': np.mean(act_vals),
        })

    results_df = pd.DataFrame(results)

    if len(results_df) > 0:
        # FDR correction
        from statsmodels.stats.multitest import multipletests
        _, results_df['pearson_q'], _, _ = multipletests(
            results_df['pearson_p'], method='fdr_bh'
        )
        _, results_df['spearman_q'], _, _ = multipletests(
            results_df['spearman_p'], method='fdr_bh'
        )

    log(f"  Validated {len(results_df)} signatures")

    # Save results
    output_path.parent.mkdir(parents=True, exist_ok=True)
    results_df.to_csv(output_path, index=False)
    log(f"  Saved: {output_path}")

    return results_df


# ==============================================================================
# Main
# ==============================================================================

def run_inference_and_validation(pseudobulk_key: str, signature: str) -> Dict:
    """Run activity inference and validation for a specific combination."""

    pseudobulk_file = PSEUDOBULK_FILES.get(pseudobulk_key)
    if not pseudobulk_file:
        raise ValueError(f"Unknown pseudobulk key: {pseudobulk_key}")

    pseudobulk_path = PSEUDOBULK_ROOT / pseudobulk_file

    if not pseudobulk_path.exists():
        raise FileNotFoundError(f"Pseudobulk file not found: {pseudobulk_path}")

    activity_path = OUTPUT_ROOT / f"{pseudobulk_key}_{signature}_activity.h5ad"
    validation_path = OUTPUT_ROOT / f"{pseudobulk_key}_{signature}_validation.csv"

    log("=" * 60)
    log(f"Activity Inference: {pseudobulk_key} / {signature}")
    log("=" * 60)

    start_time = time.time()

    # Run inference
    inference_summary = run_activity_inference(
        pseudobulk_path=pseudobulk_path,
        signature_name=signature,
        output_path=activity_path,
    )

    # Run validation
    validation_df = validate_expression_vs_activity(
        pseudobulk_path=pseudobulk_path,
        activity_path=activity_path,
        signature_name=signature,
        output_path=validation_path,
    )

    elapsed = time.time() - start_time

    # Summary
    summary = {
        **inference_summary,
        'validation_count': len(validation_df),
        'mean_pearson_r': validation_df['pearson_r'].mean() if len(validation_df) > 0 else None,
        'mean_spearman_r': validation_df['spearman_r'].mean() if len(validation_df) > 0 else None,
        'total_time_seconds': elapsed,
    }

    log(f"Completed in {elapsed:.1f}s")
    log(f"Mean Pearson r: {summary['mean_pearson_r']:.3f}" if summary['mean_pearson_r'] else "No validation results")

    return summary


def main():
    parser = argparse.ArgumentParser(description="Run activity inference on pseudobulk data")
    parser.add_argument('--pseudobulk', type=str, help='Pseudobulk key (e.g., cima_L1)')
    parser.add_argument('--signature', type=str, choices=SIGNATURES, help='Signature type')
    parser.add_argument('--all', action='store_true', help='Run all combinations')
    parser.add_argument('--config-index', type=int, help='Config index for SLURM array')

    args = parser.parse_args()

    # Define all combinations
    ALL_CONFIGS = []
    for pb_key in PSEUDOBULK_FILES.keys():
        for sig in SIGNATURES:
            ALL_CONFIGS.append((pb_key, sig))

    if args.config_index is not None:
        if args.config_index >= len(ALL_CONFIGS):
            print(f"Config index {args.config_index} out of range (max {len(ALL_CONFIGS) - 1})")
            sys.exit(1)
        pb_key, sig = ALL_CONFIGS[args.config_index]
        run_inference_and_validation(pb_key, sig)
    elif args.all:
        for pb_key, sig in ALL_CONFIGS:
            try:
                run_inference_and_validation(pb_key, sig)
            except Exception as e:
                log(f"ERROR: {pb_key}/{sig}: {e}")
    elif args.pseudobulk and args.signature:
        run_inference_and_validation(args.pseudobulk, args.signature)
    else:
        parser.print_help()
        print(f"\nAvailable pseudobulk keys: {list(PSEUDOBULK_FILES.keys())}")
        print(f"Available signatures: {SIGNATURES}")
        print(f"\nTotal configs: {len(ALL_CONFIGS)}")


if __name__ == "__main__":
    main()
